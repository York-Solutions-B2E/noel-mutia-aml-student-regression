{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "# Load CSV data\n",
    "csv_path = './docs/data.csv'\n",
    "df = pd.read_csv(csv_path, delimiter=';')\n",
    "\n",
    "# Exploring the data\n",
    "print(df.head(10))  # Display first few rows\n",
    "print(df.describe())  # Display summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "df.isnull().sum()\n",
    "drop_df = df.dropna()\n",
    "print(drop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().value_counts())\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"G3\", axis = 1)\n",
    "print(X.describe())\n",
    "y = df[\"G3\"]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian',\n",
    "    'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic'\n",
    "]\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to categorical columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "# Display the DataFrame after label encoding\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_df = df.copy()\n",
    "\n",
    "corr_matrix = EDA_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', annot_kws={\"size\": 10})\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_df.hist(bins=20, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "X = df.dropna(axis=0)\n",
    "X = X.drop(\"G3\", axis = 1)\n",
    "y = df[\"G3\"]\n",
    "\n",
    "# Create new features\n",
    "X['academic_performance'] = X['G1'] + X['G2']\n",
    "X['study_engagement'] = X['studytime'] * X['absences']\n",
    "X['social_life'] = X['goout'] * X['freetime']\n",
    "X['family_support'] = X['famsup'] * X['famrel']\n",
    "X['health_and_activities'] = X['health'] * X['activities']\n",
    "X['alcohol_consumption'] = X['Dalc'] + X['Walc']\n",
    "X['study_habits'] = X['studytime'] * X['failures']\n",
    "X['parental_education'] = X['Medu'] + X['Fedu']\n",
    "X['romantic_relationship'] = 1 - X['romantic']\n",
    "X['internet_and_higher_edu'] = X['internet'] * X['higher']\n",
    "\n",
    "# All discrete features should now have integer dtypes\n",
    "discrete_features = X.dtypes == int\n",
    "\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "print(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores, color='salmon')\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "# Assuming mi_scores is the mutual information scores you calculated\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_mi_scores(mi_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of subsets to evaluate\n",
    "featureSets = [\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'romantic'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob', 'sex'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob', 'sex', 'alcohol_consumption'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob', 'sex', 'alcohol_consumption', 'paid'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob', 'sex', 'alcohol_consumption', 'paid', 'schoolsup']\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Feature Set', 'Model', 'r2', 'MSE', 'mean_absolute_percentage'])\n",
    "\n",
    "for features in featureSets:\n",
    "    try:\n",
    "        X_features = X[features]\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(scaler.fit_transform(X_features), columns=X_features.columns)\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Define models\n",
    "        models = {\n",
    "          'KNN Regressor': KNeighborsRegressor(),\n",
    "          'Linear SVR': SVR(),\n",
    "          'Ridge Regressor': Ridge()\n",
    "        }\n",
    "\n",
    "        # Train and evaluate models\n",
    "        for model_name, model in models.items():\n",
    "          model.fit(X_train, y_train)\n",
    "          y_pred = model.predict(X_test)\n",
    "          mse = mean_squared_error(y_test, y_pred)\n",
    "          r2 = r2_score(y_test, y_pred)\n",
    "          mean_absolute_percentage = mean_absolute_error(y_test, y_pred)\n",
    "          results_df.loc[len(results_df.index)] = [', '.join(features), model_name, r2, mse, mean_absolute_percentage]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing feature set {', '.join(features)}: {e}\")\n",
    "\n",
    "# Display results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.suptitle(\"Mean stats per feature set\")\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(results_df['Feature Set'], results_df['mean_absolute_percentage'])\n",
    "plt.ylim(0.1)\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.title(\"Percent Error\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(results_df['Feature Set'], results_df['r2'])\n",
    "plt.ylim(0.6)\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.title(\"r2\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(results_df['Feature Set'], results_df['MSE'])\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.title(\"Mean Squared Error\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 18), sharex=True)\n",
    "\n",
    "# Bar plot for R-squared (r2)\n",
    "sns.barplot(x='Model', y='r2', hue='Feature Set', data=results_df, ax=ax1)\n",
    "ax1.set_ylabel('R-squared (r2) Value')\n",
    "ax1.set_title('Comparison of R-squared Values by Feature Set and Model')\n",
    "\n",
    "# Bar plot for Mean Squared Error (MSE)\n",
    "sns.barplot(x='Model', y='MSE', hue='Feature Set', data=results_df, ax=ax2)\n",
    "ax2.set_ylabel('Mean Squared Error (MSE) Value')\n",
    "ax2.set_title('Comparison of MSE Values by Feature Set and Model')\n",
    "\n",
    "# Bar plot for Mean Absolute Percentage Error (MAPE)\n",
    "sns.barplot(x='Model', y='mean_absolute_percentage', hue='Feature Set', data=results_df, ax=ax3)\n",
    "ax3.set_xlabel('Model')\n",
    "ax3.set_ylabel('Mean Absolute Percentage Error (MAPE) Value')\n",
    "ax3.set_title('Comparison of MAPE Values by Feature Set and Model')\n",
    "\n",
    "# Move the legend to the upper right corner\n",
    "ax1.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax2.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax3.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.suptitle(\"Mean stats per model type\")\n",
    "\n",
    "# Subplot 1: Percent Error\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(results_df['Feature Set'], results_df['mean_absolute_percentage'])\n",
    "plt.ylim(0.1)\n",
    "plt.title(\"Percent Error\")\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Subplot 2: r2\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(results_df['Feature Set'], results_df['r2'])\n",
    "plt.ylim(0.6)\n",
    "plt.title(\"r2\")\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Subplot 3: Mean Squared Error\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(results_df['Feature Set'], results_df['MSE'])\n",
    "plt.title(\"Mean Squared Error\")\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for KNN Regressor\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Create KNN Regressor model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_results_df = pd.DataFrame(columns=['Feature Set', 'Model', 'r2', 'MSE', 'mean_absolute_percentage'])\n",
    "\n",
    "# Iterate through feature sets\n",
    "for features in featureSets:\n",
    "    try:\n",
    "        X_features = X[features]\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(scaler.fit_transform(X_features), columns=X_features.columns)\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Perform GridSearchCV\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best hyperparameters\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Use the best hyperparameters to train the final model\n",
    "        final_model = KNeighborsRegressor(**best_params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the final model\n",
    "        y_pred = final_model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mean_absolute_percentage = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "        # Log results\n",
    "        grid_results_df.loc[len(grid_results_df.index)] = [', '.join(features), 'KNN Regressor', r2, mse, mean_absolute_percentage]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing feature set {', '.join(features)}: {e}\")\n",
    "\n",
    "# Display results\n",
    "print(grid_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.suptitle(\"Mean stats per feature set\")\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(grid_results_df['Feature Set'], grid_results_df['mean_absolute_percentage'])\n",
    "plt.ylim(0.1)\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.title(\"Percent Error\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(grid_results_df['Feature Set'], grid_results_df['r2'])\n",
    "plt.ylim(0.6)\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.title(\"r2\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(grid_results_df['Feature Set'], grid_results_df['MSE'])\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.title(\"Mean Squared Error\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
