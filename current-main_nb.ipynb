{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, max_error, median_absolute_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "import csv\n",
    "import os\n",
    "import psycopg2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer the Data to Postgresql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Start by storing our data from the csv file in a dataframe\n",
    "#### After inspecting the data we notice that the columns are seperated with a ';' semi colon, to solve this we use delimiter to seperate the columns in our dataframe. \n",
    "#### The print statements will give us a glimpse of the code structure and a brief summary about each columns statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./docs/data.csv\", delimiter=';')\n",
    "print(df.head(10))  # Display first few rows\n",
    "print(df.describe())  # Display summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we will have two functions that will do the following:\n",
    "1. The first function (load_data_to_postgres_docker) will load the data from the csv file into the database\n",
    "2. The second function (add_new_student) will add a new student row with information provided to the postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_postgres_docker(table_name, user, password, host='mypostgres', port=5432, database_name='student_grades'):\n",
    "    # Create engine with SQLAlchemy\n",
    "    password = quote_plus(password)\n",
    "    engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{database_name}')\n",
    "\n",
    "    # Transfer data to database\n",
    "    df.to_sql(table_name, engine, index=False, if_exists='replace')\n",
    "\n",
    "# load_data_to_postgres_docker('student_grades_table', 'leonphoenix21', 'vtlt123@dck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_student(student_info, table_name, database_name, user, password, host='127.0.0.1', port=5432):\n",
    "    # Create engine with SQLAlchemy\n",
    "    password = quote_plus(password)\n",
    "    engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{database_name}')\n",
    "\n",
    "    # Convert student_info to DataFrame\n",
    "    new_student_data = pd.DataFrame([student_info])\n",
    "\n",
    "    # Append new student data to the PostgreSQL table\n",
    "    new_student_data.to_sql(table_name, engine, index=False, if_exists='append')\n",
    "\n",
    "new_student_info = {\n",
    "    'school': 'GP',\n",
    "    'sex': 'M',\n",
    "    'age': 16,\n",
    "    'address': 'U',\n",
    "    'famsize': 'LE3',\n",
    "    'Pstatus': 'T',\n",
    "    'Medu': 4,\n",
    "    'Fedu': 3,\n",
    "    'Mjob': 'teacher',\n",
    "    'Fjob': 'services',\n",
    "    'reason': 'course',\n",
    "    'guardian': 'mother',\n",
    "    'traveltime': 3,\n",
    "    'studytime': 2,\n",
    "    'failures': 0,\n",
    "    'schoolsup': 'no',\n",
    "    'famsup': 'yes',\n",
    "    'paid': 'no',\n",
    "    'activities': 'yes',\n",
    "    'nursery': 'yes',\n",
    "    'higher': 'yes',\n",
    "    'internet': 'yes',\n",
    "    'romantic': 'no',\n",
    "    'famrel': 5,\n",
    "    'freetime': 4,\n",
    "    'goout': 3,\n",
    "    'Dalc': 1,\n",
    "    'Walc': 2,\n",
    "    'health': 1,\n",
    "    'absences': 2,\n",
    "    'G1': 15,\n",
    "    'G2': 14,\n",
    "    'G3': 15\n",
    "}\n",
    "\n",
    "# add_new_student(new_student_info, 'student_grades_table', 'student_grades', 'leonphoenix21', 'vtlt123@dck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are going to Clean, process and explore the data, by calculating the frequencies and mutual information score for each feature as well creating visualizations that will give an insight into the correlation between features. Allowing us to analyze and better understand the relationship of the features to the target feature.\n",
    "#### The Code Blocks below should accomplish the following tasks:\n",
    "+ Handle missing values \n",
    "* We will check for any inconsistent data entry any duplicates e.t.c\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values.\n",
    "#### First Section here checks for the missing values and then we print all the columns and the number of missing values since we have a smaller database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# how many total missing values do we have?\n",
    "total_cells = np.product(df.shape)\n",
    "total_missing = missing_values.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "percent_missing = (total_missing/total_cells) * 100\n",
    "print(percent_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicated rows\n",
    "Now we move on to checking for duplicate rows in our dataframe and drop them if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().value_counts())\n",
    "df = df.drop_duplicates()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are about to Train our model. First in the code block below I create a copy of the data without the G3 column and we then clone the G3 column in the y-variable so that we can compare our predictions to the actual results from our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"G3\", axis = 1)\n",
    "print(X.describe())\n",
    "y = df[\"G3\"]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Here I am defining my categorical columns and I am going to encode those columns using a label encoder. I am making a note to encode the X variables and not the main (df) dataframe variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns\n",
    "categorical_columns = [\n",
    "    'school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian',\n",
    "    'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic'\n",
    "]\n",
    "\n",
    "# Apply label encoding to categorical columns in X\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# Display the DataFrame after label encoding\n",
    "print(X.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I plan to create visualizations that will help us with our feature selection by seeing the correlation between the features, the spread of each feature and the their shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_df = X.copy()\n",
    "\n",
    "corr_matrix = EDA_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix,  cmap='coolwarm', annot_kws={\"size\": 5})\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.hist(bins=20, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking through the visualizations and exploring the data, I took a note of the performance of all the features and decided to create 10 new features as well as 10 feature subsets to evaluate and find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "X['academic_performance'] = X['G1'] + X['G2']\n",
    "X['study_engagement'] = X['studytime'] * X['absences']\n",
    "X['social_life'] = X['goout'] * X['freetime']\n",
    "X['family_support'] = X['famsup'] * X['famrel']\n",
    "X['health_and_activities'] = X['health'] * X['activities']\n",
    "X['alcohol_consumption'] = X['Dalc'] + X['Walc']\n",
    "X['study_habits'] = X['studytime'] * X['failures']\n",
    "X['parental_education'] = X['Medu'] + X['Fedu']\n",
    "X['internet_higher_edu'] = X['internet'] * X['higher']\n",
    "X['Walc_health'] = X['Walc'] * X['health']\n",
    "\n",
    "\n",
    "\n",
    "# All discrete features should now have integer dtypes\n",
    "discrete_features = X.dtypes == int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created new features we can calculate our mutual information score and create a visualization for each feature to find the best performing features to use that information for creating our featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "print(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores, color='salmon')\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "# Assuming mi_scores is the mutual information scores you calculated\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_mi_scores(mi_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After taking a look at the MI scores it's apparent that only a few features are highly dependent on our target variable (G3). So using that information we can now create an array of feature sets to test against our model. I chose to use the high performing features first and then to add more features to test which one work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of subsets to evaluate\n",
    "featureSets = [\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'romantic'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob', 'sex'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob', 'sex', 'alcohol_consumption'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob', 'sex', 'alcohol_consumption', 'paid'],\n",
    "  ['G2', 'academic_performance', 'G1', 'study_engagement', 'failures', 'Mjob', 'Walc', 'Fjob', 'sex', 'alcohol_consumption', 'paid', 'schoolsup']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created new features and tested the performance and selected our features for our featureset. We will now be training our data using 3 different models to see which model will perform best with our features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame(columns=['Feature Set', 'Model', 'r2', 'MSE', 'mean_absolute_percentage', 'max_error', 'median_absolute_error'])\n",
    "\n",
    "for features in featureSets:\n",
    "    try:\n",
    "        X_features = X[features]\n",
    "        print(X_features.head(1))\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(scaler.fit_transform(X_features), columns=X_features.columns)\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Define models\n",
    "        models = {\n",
    "            'KNN Regressor': KNeighborsRegressor(),\n",
    "            'Linear SVR': SVR(),\n",
    "            'Ridge Regressor': Ridge()\n",
    "        }\n",
    "\n",
    "        # Train and evaluate models\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mean_absolute_percentage = mean_absolute_error(y_test, y_pred)\n",
    "            max_err = max_error(y_test, y_pred)\n",
    "            medianae = median_absolute_error(y_test, y_pred)\n",
    "\n",
    "            results_df.loc[len(results_df.index)] = [', '.join(features), model_name, r2, mse, mean_absolute_percentage, max_err, medianae]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing feature set {', '.join(features)}: {e}\")\n",
    "\n",
    "# Display results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our trained our models on our feature sets lets create some visualizations displaying the model performance performance by measuring different important metrics that tell us how accurate our predictions are. We can gain insights from this process to understand how we can fine-tune the model for enhanced performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(25,15))\n",
    "plt.suptitle(\"Mean stats per model type\")\n",
    "\n",
    "# Subplot 1: Mean Absolute Error\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.bar(results_df['Feature Set'], results_df['mean_absolute_percentage'])\n",
    "plt.ylim(0.1)\n",
    "plt.title(\"Mean Absolute Error\")\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Subplot 2: r2\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.bar(results_df['Feature Set'], results_df['r2'])\n",
    "plt.ylim(0.6)\n",
    "plt.title(\"r2\")\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Subplot 3: Mean Squared Error\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.bar(results_df['Feature Set'], results_df['MSE'])\n",
    "plt.title(\"Mean Squared Error\")\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Subplot 4: Max Error\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.bar(results_df['Feature Set'], results_df['max_error'])\n",
    "plt.title(\"Max Error\")\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Subplot 5: Median Absolute Error\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.bar(results_df['Feature Set'], results_df['median_absolute_error'])\n",
    "plt.title(\"Median Absolute Error\")\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(5, 1, figsize=(15, 24), sharex=True)\n",
    "\n",
    "# Bar plot for R-squared (r2)\n",
    "sns.barplot(x='Model', y='r2', hue='Feature Set', data=results_df, ax=axes[0])\n",
    "axes[0].set_ylabel('R-squared (r2) Value')\n",
    "axes[0].set_title('Comparison of R-squared Values by Feature Set and Model')\n",
    "\n",
    "# Bar plot for Mean Squared Error (MSE)\n",
    "sns.barplot(x='Model', y='MSE', hue='Feature Set', data=results_df, ax=axes[1])\n",
    "axes[1].set_ylabel('Mean Squared Error (MSE) Value')\n",
    "axes[1].set_title('Comparison of MSE Values by Feature Set and Model')\n",
    "\n",
    "# Bar plot for Mean Absolute Percentage Error (MAPE)\n",
    "sns.barplot(x='Model', y='mean_absolute_percentage', hue='Feature Set', data=results_df, ax=axes[2])\n",
    "axes[2].set_ylabel('Mean Absolute Percentage Error (MAPE) Value')\n",
    "axes[2].set_title('Comparison of MAPE Values by Feature Set and Model')\n",
    "\n",
    "# Bar plot for Max Error\n",
    "sns.barplot(x='Model', y='max_error', hue='Feature Set', data=results_df, ax=axes[3])\n",
    "axes[3].set_ylabel('Max Error Value')\n",
    "axes[3].set_title('Comparison of Max Error Values by Feature Set and Model')\n",
    "\n",
    "# Bar plot for Median Absolute Error\n",
    "sns.barplot(x='Model', y='median_absolute_error', hue='Feature Set', data=results_df, ax=axes[4])\n",
    "axes[4].set_xlabel('Model')\n",
    "axes[4].set_ylabel('Median Absolute Error Value')\n",
    "axes[4].set_title('Comparison of Median Absolute Error Values by Feature Set and Model')\n",
    "\n",
    "# Move the legend to the upper right corner\n",
    "for ax in axes:\n",
    "    ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the performance of the model, We can now fine tune the hyper-paramaters. I am first going to fine tune the parameters with Grid Search, and I chose this because our dataset isn't huge and the grid search should be a good start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the parameter grid for KNN Regressor\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "grid_results_df = pd.DataFrame(columns=['Feature Set', 'Model', 'r2', 'MSE', 'mean_absolute_percentage', 'max_error', 'median_absolute_error'])\n",
    "\n",
    "# Iterate through feature sets\n",
    "for features in featureSets:\n",
    "    try:\n",
    "        X_features = X[features]\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(scaler.fit_transform(X_features), columns=X_features.columns)\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Create KNN Regressor model\n",
    "        knn_model = KNeighborsRegressor()\n",
    "\n",
    "        # Create GridSearchCV object\n",
    "        grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "        # Perform GridSearchCV\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best hyperparameters\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Use the best hyperparameters to train the final model\n",
    "        final_model = KNeighborsRegressor(**best_params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the final model\n",
    "        y_pred = final_model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mean_absolute_percentage = mean_absolute_error(y_test, y_pred)\n",
    "        max_err = max_error(y_test, y_pred)\n",
    "        medianae = median_absolute_error(y_test, y_pred)\n",
    "\n",
    "        # Log results\n",
    "        grid_results_df.loc[len(grid_results_df.index)] = [', '.join(features), 'KNN Regressor', r2, mse, mean_absolute_percentage, max_err, medianae]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing feature set {', '.join(features)}: {e}\")\n",
    "\n",
    "# Display results\n",
    "print(grid_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(5, 1, figsize=(15, 24), sharex=True)\n",
    "\n",
    "# Bar plot for R-squared (r2)\n",
    "sns.barplot(x='Model', y='r2', hue='Feature Set', data=grid_results_df, ax=axes[0])\n",
    "axes[0].set_ylabel('R-squared (r2) Value')\n",
    "axes[0].set_title('Comparison of R-squared Values by Feature Set and Model (GridSearchCV)')\n",
    "\n",
    "# Bar plot for Mean Squared Error (MSE)\n",
    "sns.barplot(x='Model', y='MSE', hue='Feature Set', data=grid_results_df, ax=axes[1])\n",
    "axes[1].set_ylabel('Mean Squared Error (MSE) Value')\n",
    "axes[1].set_title('Comparison of MSE Values by Feature Set and Model (GridSearchCV)')\n",
    "\n",
    "# Bar plot for Mean Absolute Percentage Error (MAPE)\n",
    "sns.barplot(x='Model', y='mean_absolute_percentage', hue='Feature Set', data=grid_results_df, ax=axes[2])\n",
    "axes[2].set_ylabel('Mean Absolute Percentage Error (MAPE) Value')\n",
    "axes[2].set_title('Comparison of MAPE Values by Feature Set and Model (GridSearchCV)')\n",
    "\n",
    "# Bar plot for Max Error\n",
    "sns.barplot(x='Model', y='max_error', hue='Feature Set', data=grid_results_df, ax=axes[3])\n",
    "axes[3].set_ylabel('Max Error Value')\n",
    "axes[3].set_title('Comparison of Max Error Values by Feature Set and Model (GridSearchCV)')\n",
    "\n",
    "# Bar plot for Median Absolute Error\n",
    "sns.barplot(x='Model', y='median_absolute_error', hue='Feature Set', data=grid_results_df, ax=axes[4])\n",
    "axes[4].set_xlabel('Model')\n",
    "axes[4].set_ylabel('Median Absolute Error Value')\n",
    "axes[4].set_title('Comparison of Median Absolute Error Values by Feature Set and Model (GridSearchCV)')\n",
    "\n",
    "# Move the legend to the upper right corner\n",
    "for ax in axes:\n",
    "    ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout for better visibility\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to write another function to do a randomized hyper-parameter tuning. I just want to compare the result to that of the GridSearch and see which is the better performing tuning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the parameter distribution for KNN Regressor\n",
    "param_dist = {\n",
    "    'n_neighbors': randint(1, 10),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "random_results_df = pd.DataFrame(columns=['Feature Set', 'Model', 'r2', 'MSE', 'mean_absolute_percentage', 'max_error', 'median_absolute_error'])\n",
    "\n",
    "# Iterate through feature sets\n",
    "for features in featureSets:\n",
    "    try:\n",
    "        X_features = X[features]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(scaler.fit_transform(X_features), columns=X_features.columns)\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        knn_model = KNeighborsRegressor()\n",
    "\n",
    "        # Create RandomizedSearchCV object\n",
    "        random_search = RandomizedSearchCV(knn_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "        # Perform RandomizedSearchCV\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best hyperparameters\n",
    "        best_params = random_search.best_params_\n",
    "\n",
    "        # Use the best hyperparameters to train the final model\n",
    "        final_model = KNeighborsRegressor(**best_params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the final model\n",
    "        y_pred = final_model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mean_absolute_percentage = mean_absolute_error(y_test, y_pred)\n",
    "        max_err = max_error(y_test, y_pred)\n",
    "        medianae = median_absolute_error(y_test, y_pred)\n",
    "\n",
    "        # Log results\n",
    "        random_results_df.loc[len(random_results_df.index)] = [', '.join(features), 'KNN Regressor', r2, mse, mean_absolute_percentage, max_err, medianae]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing feature set {', '.join(features)}: {e}\")\n",
    "\n",
    "# Display results\n",
    "print(random_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(5, 1, figsize=(15, 24), sharex=True)\n",
    "\n",
    "# Bar plot for R-squared (r2)\n",
    "sns.barplot(x='Model', y='r2', hue='Feature Set', data=random_results_df, ax=axes[0])\n",
    "axes[0].set_ylabel('R-squared (r2) Value')\n",
    "axes[0].set_title('Comparison of R-squared Values by Feature Set and Model (RandomizedSearchCV)')\n",
    "\n",
    "# Bar plot for Mean Squared Error (MSE)\n",
    "sns.barplot(x='Model', y='MSE', hue='Feature Set', data=random_results_df, ax=axes[1])\n",
    "axes[1].set_ylabel('Mean Squared Error (MSE) Value')\n",
    "axes[1].set_title('Comparison of MSE Values by Feature Set and Model (RandomizedSearchCV)')\n",
    "\n",
    "# Bar plot for Mean Absolute Percentage Error (MAPE)\n",
    "sns.barplot(x='Model', y='mean_absolute_percentage', hue='Feature Set', data=random_results_df, ax=axes[2])\n",
    "axes[2].set_ylabel('Mean Absolute Percentage Error (MAPE) Value')\n",
    "axes[2].set_title('Comparison of MAPE Values by Feature Set and Model (RandomizedSearchCV)')\n",
    "\n",
    "# Bar plot for Max Error\n",
    "sns.barplot(x='Model', y='max_error', hue='Feature Set', data=random_results_df, ax=axes[3])\n",
    "axes[3].set_ylabel('Max Error Value')\n",
    "axes[3].set_title('Comparison of Max Error Values by Feature Set and Model (RandomizedSearchCV)')\n",
    "\n",
    "# Bar plot for Median Absolute Error\n",
    "sns.barplot(x='Model', y='median_absolute_error', hue='Feature Set', data=random_results_df, ax=axes[4])\n",
    "axes[4].set_xlabel('Model')\n",
    "axes[4].set_ylabel('Median Absolute Error Value')\n",
    "axes[4].set_title('Comparison of Median Absolute Error Values by Feature Set and Model (RandomizedSearchCV)')\n",
    "\n",
    "# Move the legend to the upper right corner\n",
    "for ax in axes:\n",
    "    ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout for better visibility\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Requests (Bonus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
